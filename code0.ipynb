{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch pandas numpy tqdm hazm emoji -q\n",
        "!pip install accelerate -U -q\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "print(\"all the libraries are installed\")"
      ],
      "metadata": {
        "id": "mlALVD3HAjon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip uninstall -y transformers torch\n"
      ],
      "metadata": {
        "id": "FzWT6QJJBP3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==2.1.1\n",
        "!pip install torch==2.4.1\n",
        "!pip install transformers==4.45.2\n",
        "!pip install accelerate==1.1.1\n",
        "!pip install pandas tqdm hazm emoji\n"
      ],
      "metadata": {
        "id": "PR7_fq8qBRqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n"
      ],
      "metadata": {
        "id": "dgWrb9BaE562"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\n"
      ],
      "metadata": {
        "id": "CVr1L09sE54P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.45.2 accelerate==1.1.1 numpy==2.1.1 pandas tqdm hazm emoji\n"
      ],
      "metadata": {
        "id": "VKGzflrfE51u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install transformers torch pandas tqdm -q\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "tqdm.pandas()\n",
        "\n",
        "\n",
        "class PersianTweetLabeler:\n",
        "    def __init__(self, model_name=\"dadashzadeh/roberta-sentiment-persian\"):\n",
        "        self.model_name = model_name\n",
        "        self.device = 0 if torch.cuda.is_available() else -1\n",
        "        print(f\"🔄 در حال بارگذاری مدل: {model_name}\")\n",
        "\n",
        "\n",
        "        self.classifier = pipeline(\n",
        "            \"text-classification\",\n",
        "            model=model_name,\n",
        "            tokenizer=model_name,\n",
        "            device=self.device,\n",
        "            truncation=True,\n",
        "            max_length=256\n",
        "        )\n",
        "        print(\"✅ مدل با موفقیت بارگذاری شد!\\n\")\n",
        "\n",
        "    def label_tweets_batch(self, tweets, batch_size=32):\n",
        "        \"\"\"لیبل‌زنی دسته‌ای با مدیریت حافظه و خطا\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for i in tqdm(range(0, len(tweets), batch_size), desc=\"📊 لیبل‌زنی\"):\n",
        "            batch = tweets[i:i + batch_size]\n",
        "\n",
        "            try:\n",
        "                predictions = self.classifier(batch)\n",
        "                for tweet, pred in zip(batch, predictions):\n",
        "                    results.append({\n",
        "                        'text': tweet,\n",
        "                        'label': pred['label'],\n",
        "                        'confidence': round(pred['score'], 4),\n",
        "                        'batch_id': i // batch_size\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ خطا در دسته {i}: {e}\")\n",
        "\n",
        "                for tweet in batch:\n",
        "                    try:\n",
        "                        pred = self.classifier(tweet)[0]\n",
        "                        results.append({\n",
        "                            'text': tweet,\n",
        "                            'label': pred['label'],\n",
        "                            'confidence': round(pred['score'], 4),\n",
        "                            'batch_id': i // batch_size\n",
        "                        })\n",
        "                    except:\n",
        "                        results.append({\n",
        "                            'text': tweet,\n",
        "                            'label': 'ERROR',\n",
        "                            'confidence': 0.0,\n",
        "                            'batch_id': i // batch_size\n",
        "                        })\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "labeler = PersianTweetLabeler()\n",
        "\n",
        "sample_texts = [\n",
        "    \"امروز خیلی خوشحالم 😊\",\n",
        "    \"از این وضعیت خسته شدم 😞\",\n",
        "    \"این محصول معمولی بود، نه خوب نه بد.\"\n",
        "]\n",
        "df = labeler.label_tweets_batch(sample_texts, batch_size=2)\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "d6MTerCQIa8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "print(\"Setting up Persian dataset preprocessor...\")\n",
        "\n",
        "class PersianDatasetPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.processed_data = None\n",
        "        print(\"Preprocessor ready!\")\n",
        "\n",
        "    def upload_dataset(self):\n",
        "        print(\"Please upload your dataset file (CSV, Excel, JSON)...\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if not uploaded:\n",
        "            print(\"No file uploaded!\")\n",
        "            return None\n",
        "\n",
        "        file_name = list(uploaded.keys())[0]\n",
        "        print(f\"File '{file_name}' uploaded successfully\")\n",
        "\n",
        "        try:\n",
        "            if file_name.endswith('.csv'):\n",
        "                df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "            elif file_name.endswith(('.xlsx', '.xls')):\n",
        "                df = pd.read_excel(io.BytesIO(uploaded[file_name]))\n",
        "            elif file_name.endswith('.json'):\n",
        "                df = pd.read_json(io.BytesIO(uploaded[file_name]))\n",
        "            else:\n",
        "                df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "\n",
        "            print(f\"Dataset loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file: {e}\")\n",
        "            return None\n",
        "\n",
        "    def clean_persian_text(self, text):\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        text = str(text)\n",
        "\n",
        "        text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "\n",
        "        text = re.sub(r'[‌\\u200c]+', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def analyze_dataset(self, df):\n",
        "        print(\"\\nDataset Analysis:\")\n",
        "        print(f\"   Dimensions: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
        "        print(f\"   Columns: {list(df.columns)}\")\n",
        "\n",
        "        print(f\"\\nColumn Information:\")\n",
        "        for col in df.columns:\n",
        "            null_count = df[col].isnull().sum()\n",
        "            sample_value = df[col].iloc[0] if len(df) > 0 else \"N/A\"\n",
        "            print(f\"   ├─ {col}: {null_count} null values | Sample: {str(sample_value)[:50]}...\")\n",
        "\n",
        "    def find_text_column(self, df):\n",
        "        text_keywords = ['text', 'tweet', 'comment', 'review', 'نظر', 'متن', 'توئیت', 'کامنت']\n",
        "\n",
        "        for col in df.columns:\n",
        "            col_lower = col.lower()\n",
        "            if any(keyword in col_lower for keyword in text_keywords):\n",
        "                return col\n",
        "\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object':\n",
        "                return col\n",
        "\n",
        "        return df.columns[0]\n",
        "\n",
        "    def preprocess(self, df, text_column=None):\n",
        "        print(\"Starting preprocessing...\")\n",
        "\n",
        "        if text_column is None:\n",
        "            text_column = self.find_text_column(df)\n",
        "            print(f\"   Text column identified: '{text_column}'\")\n",
        "\n",
        "        processed_df = df.copy()\n",
        "\n",
        "        initial_count = len(processed_df)\n",
        "        processed_df = processed_df.dropna(how='all')\n",
        "        print(f\"   Removed completely empty rows: {initial_count - len(processed_df)} rows\")\n",
        "\n",
        "        print(f\"   Cleaning column '{text_column}'...\")\n",
        "        processed_df['cleaned_text'] = processed_df[text_column].apply(self.clean_persian_text)\n",
        "\n",
        "        initial_text_count = len(processed_df)\n",
        "        processed_df = processed_df[processed_df['cleaned_text'].str.len() > 10]\n",
        "        short_text_removed = initial_text_count - len(processed_df)\n",
        "        print(f\"   Removed short texts: {short_text_removed} rows\")\n",
        "\n",
        "        initial_unique = len(processed_df)\n",
        "        processed_df = processed_df.drop_duplicates(subset=['cleaned_text'])\n",
        "        duplicates_removed = initial_unique - len(processed_df)\n",
        "        print(f\"   Removed duplicates: {duplicates_removed} rows\")\n",
        "\n",
        "        print(f\"\\nPreprocessing completed!\")\n",
        "        print(f\"   Before: {len(df)} rows\")\n",
        "        print(f\"   After: {len(processed_df)} rows\")\n",
        "        print(f\"   Removed: {len(df) - len(processed_df)} rows\")\n",
        "\n",
        "        self.processed_data = processed_df\n",
        "        return processed_df\n",
        "\n",
        "    def show_samples(self, n=5):\n",
        "        if self.processed_data is not None:\n",
        "            print(f\"\\n{n} samples of processed data:\")\n",
        "            samples = self.processed_data[['cleaned_text']].head(n)\n",
        "            for i, (idx, row) in enumerate(samples.iterrows()):\n",
        "                print(f\"   {i+1}. {row['cleaned_text'][:80]}...\")\n",
        "        else:\n",
        "            print(\"No data to display!\")\n",
        "\n",
        "preprocessor = PersianDatasetPreprocessor()\n",
        "\n",
        "df = preprocessor.upload_dataset()\n",
        "\n",
        "if df is not None:\n",
        "    preprocessor.analyze_dataset(df)\n",
        "\n",
        "    processed_df = preprocessor.preprocess(df)\n",
        "\n",
        "    preprocessor.show_samples(5)\n",
        "\n",
        "    print(\"\\nDataset is ready! You can use processed_df.\")\n",
        "else:\n",
        "    print(\"Operation stopped!\")"
      ],
      "metadata": {
        "id": "wQKMUrWXIa2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cleaned_tweets = preprocessor.processed_data['cleaned_text'].tolist()\n",
        "\n",
        "print(f\"Number of cleaned tweets: {len(cleaned_tweets)}\")\n",
        "\n",
        "preprocessor.processed_data.to_csv('cleaned_dataset.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('cleaned_dataset.csv')"
      ],
      "metadata": {
        "id": "tghg-P37Iaz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "model_name = \"HooshvareLab/bert-fa-base-uncased\"\n",
        "classifier = pipeline(\"text-generation\", model=model_name, tokenizer=model_name)\n",
        "\n",
        "prompt_template = '''وظیفه: توییت‌های فارسی را به ۳ کلاس برچسب بزن: 0=منفی،1=خنثی،2=مثبت.\n",
        "مثال‌ها:\n",
        "\"این محصول افتضاحه، پولمو دور ریختم\" => 0\n",
        "\"امروز هم مثل دیروز بود، چیزی خاص نبود\" => 1\n",
        "\"آفرین! چه سرویس سریعی داشتند 😊\" => 2\n",
        "\n",
        "متن: \"{}\"\n",
        "پاسخ فقط عدد کلاس.'''\n",
        "\n",
        "\n",
        "tweets = [\n",
        "    \"خیلی از این محصول راضی بودم 😍\",\n",
        "    \"واقعا خسته شدم از این وضعیت\",\n",
        "    \"امروز معمولی بود، نه بد بود نه خوب\"\n",
        "]\n",
        "\n",
        "pred_labels = []\n",
        "\n",
        "for tweet in tweets:\n",
        "    prompt = prompt_template.format(tweet)\n",
        "    output = classifier(prompt, max_new_tokens=5)[0]['generated_text']\n",
        "\n",
        "    match = re.search(r\"\\b([012])\\b\", output)\n",
        "    label = int(match.group(1)) if match else None\n",
        "    pred_labels.append(label)\n",
        "\n",
        "for t, l in zip(tweets, pred_labels):\n",
        "    print(f\"{t} => {l}\")\n"
      ],
      "metadata": {
        "id": "qRrgHRkFc9Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QCblrLjoc9Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xRgR-jM5c9Ep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}