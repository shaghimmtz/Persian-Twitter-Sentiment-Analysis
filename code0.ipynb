{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch pandas numpy tqdm hazm emoji -q\n",
        "!pip install accelerate -U -q\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import numpy as np\n",
        "\n",
        "print(\"all the libraries are installed\")"
      ],
      "metadata": {
        "id": "mlALVD3HAjon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip uninstall -y transformers torch\n"
      ],
      "metadata": {
        "id": "FzWT6QJJBP3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==2.1.1\n",
        "!pip install torch==2.4.1\n",
        "!pip install transformers==4.45.2\n",
        "!pip install accelerate==1.1.1\n",
        "!pip install pandas tqdm hazm emoji\n"
      ],
      "metadata": {
        "id": "PR7_fq8qBRqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n"
      ],
      "metadata": {
        "id": "dgWrb9BaE562"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\n"
      ],
      "metadata": {
        "id": "CVr1L09sE54P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.45.2 accelerate==1.1.1 numpy==2.1.1 pandas tqdm hazm emoji\n"
      ],
      "metadata": {
        "id": "VKGzflrfE51u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install transformers torch pandas tqdm -q\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "tqdm.pandas()\n",
        "\n",
        "\n",
        "class PersianTweetLabeler:\n",
        "    def __init__(self, model_name=\"dadashzadeh/roberta-sentiment-persian\"):\n",
        "        self.model_name = model_name\n",
        "        self.device = 0 if torch.cuda.is_available() else -1\n",
        "        print(f\"ðŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„: {model_name}\")\n",
        "\n",
        "\n",
        "        self.classifier = pipeline(\n",
        "            \"text-classification\",\n",
        "            model=model_name,\n",
        "            tokenizer=model_name,\n",
        "            device=self.device,\n",
        "            truncation=True,\n",
        "            max_length=256\n",
        "        )\n",
        "        print(\"âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯!\\n\")\n",
        "\n",
        "    def label_tweets_batch(self, tweets, batch_size=32):\n",
        "        \"\"\"Ù„ÛŒØ¨Ù„â€ŒØ²Ù†ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§ÙØ¸Ù‡ Ùˆ Ø®Ø·Ø§\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for i in tqdm(range(0, len(tweets), batch_size), desc=\"ðŸ“Š Ù„ÛŒØ¨Ù„â€ŒØ²Ù†ÛŒ\"):\n",
        "            batch = tweets[i:i + batch_size]\n",
        "\n",
        "            try:\n",
        "                predictions = self.classifier(batch)\n",
        "                for tweet, pred in zip(batch, predictions):\n",
        "                    results.append({\n",
        "                        'text': tweet,\n",
        "                        'label': pred['label'],\n",
        "                        'confidence': round(pred['score'], 4),\n",
        "                        'batch_id': i // batch_size\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø³ØªÙ‡ {i}: {e}\")\n",
        "\n",
        "                for tweet in batch:\n",
        "                    try:\n",
        "                        pred = self.classifier(tweet)[0]\n",
        "                        results.append({\n",
        "                            'text': tweet,\n",
        "                            'label': pred['label'],\n",
        "                            'confidence': round(pred['score'], 4),\n",
        "                            'batch_id': i // batch_size\n",
        "                        })\n",
        "                    except:\n",
        "                        results.append({\n",
        "                            'text': tweet,\n",
        "                            'label': 'ERROR',\n",
        "                            'confidence': 0.0,\n",
        "                            'batch_id': i // batch_size\n",
        "                        })\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "labeler = PersianTweetLabeler()\n",
        "\n",
        "sample_texts = [\n",
        "    \"Ø§Ù…Ø±ÙˆØ² Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ´Ø­Ø§Ù„Ù… ðŸ˜Š\",\n",
        "    \"Ø§Ø² Ø§ÛŒÙ† ÙˆØ¶Ø¹ÛŒØª Ø®Ø³ØªÙ‡ Ø´Ø¯Ù… ðŸ˜ž\",\n",
        "    \"Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„ Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø¨ÙˆØ¯ØŒ Ù†Ù‡ Ø®ÙˆØ¨ Ù†Ù‡ Ø¨Ø¯.\"\n",
        "]\n",
        "df = labeler.label_tweets_batch(sample_texts, batch_size=2)\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "d6MTerCQIa8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "print(\"Setting up Persian dataset preprocessor...\")\n",
        "\n",
        "class PersianDatasetPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.processed_data = None\n",
        "        print(\"Preprocessor ready!\")\n",
        "\n",
        "    def upload_dataset(self):\n",
        "        print(\"Please upload your dataset file (CSV, Excel, JSON)...\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if not uploaded:\n",
        "            print(\"No file uploaded!\")\n",
        "            return None\n",
        "\n",
        "        file_name = list(uploaded.keys())[0]\n",
        "        print(f\"File '{file_name}' uploaded successfully\")\n",
        "\n",
        "        try:\n",
        "            if file_name.endswith('.csv'):\n",
        "                df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "            elif file_name.endswith(('.xlsx', '.xls')):\n",
        "                df = pd.read_excel(io.BytesIO(uploaded[file_name]))\n",
        "            elif file_name.endswith('.json'):\n",
        "                df = pd.read_json(io.BytesIO(uploaded[file_name]))\n",
        "            else:\n",
        "                df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
        "\n",
        "            print(f\"Dataset loaded: {len(df)} rows, {len(df.columns)} columns\")\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file: {e}\")\n",
        "            return None\n",
        "\n",
        "    def clean_persian_text(self, text):\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        text = str(text)\n",
        "\n",
        "        text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "\n",
        "        text = re.sub(r'[â€Œ\\u200c]+', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def analyze_dataset(self, df):\n",
        "        print(\"\\nDataset Analysis:\")\n",
        "        print(f\"   Dimensions: {df.shape[0]} rows Ã— {df.shape[1]} columns\")\n",
        "        print(f\"   Columns: {list(df.columns)}\")\n",
        "\n",
        "        print(f\"\\nColumn Information:\")\n",
        "        for col in df.columns:\n",
        "            null_count = df[col].isnull().sum()\n",
        "            sample_value = df[col].iloc[0] if len(df) > 0 else \"N/A\"\n",
        "            print(f\"   â”œâ”€ {col}: {null_count} null values | Sample: {str(sample_value)[:50]}...\")\n",
        "\n",
        "    def find_text_column(self, df):\n",
        "        text_keywords = ['text', 'tweet', 'comment', 'review', 'Ù†Ø¸Ø±', 'Ù…ØªÙ†', 'ØªÙˆØ¦ÛŒØª', 'Ú©Ø§Ù…Ù†Øª']\n",
        "\n",
        "        for col in df.columns:\n",
        "            col_lower = col.lower()\n",
        "            if any(keyword in col_lower for keyword in text_keywords):\n",
        "                return col\n",
        "\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object':\n",
        "                return col\n",
        "\n",
        "        return df.columns[0]\n",
        "\n",
        "    def preprocess(self, df, text_column=None):\n",
        "        print(\"Starting preprocessing...\")\n",
        "\n",
        "        if text_column is None:\n",
        "            text_column = self.find_text_column(df)\n",
        "            print(f\"   Text column identified: '{text_column}'\")\n",
        "\n",
        "        processed_df = df.copy()\n",
        "\n",
        "        initial_count = len(processed_df)\n",
        "        processed_df = processed_df.dropna(how='all')\n",
        "        print(f\"   Removed completely empty rows: {initial_count - len(processed_df)} rows\")\n",
        "\n",
        "        print(f\"   Cleaning column '{text_column}'...\")\n",
        "        processed_df['cleaned_text'] = processed_df[text_column].apply(self.clean_persian_text)\n",
        "\n",
        "        initial_text_count = len(processed_df)\n",
        "        processed_df = processed_df[processed_df['cleaned_text'].str.len() > 10]\n",
        "        short_text_removed = initial_text_count - len(processed_df)\n",
        "        print(f\"   Removed short texts: {short_text_removed} rows\")\n",
        "\n",
        "        initial_unique = len(processed_df)\n",
        "        processed_df = processed_df.drop_duplicates(subset=['cleaned_text'])\n",
        "        duplicates_removed = initial_unique - len(processed_df)\n",
        "        print(f\"   Removed duplicates: {duplicates_removed} rows\")\n",
        "\n",
        "        print(f\"\\nPreprocessing completed!\")\n",
        "        print(f\"   Before: {len(df)} rows\")\n",
        "        print(f\"   After: {len(processed_df)} rows\")\n",
        "        print(f\"   Removed: {len(df) - len(processed_df)} rows\")\n",
        "\n",
        "        self.processed_data = processed_df\n",
        "        return processed_df\n",
        "\n",
        "    def show_samples(self, n=5):\n",
        "        if self.processed_data is not None:\n",
        "            print(f\"\\n{n} samples of processed data:\")\n",
        "            samples = self.processed_data[['cleaned_text']].head(n)\n",
        "            for i, (idx, row) in enumerate(samples.iterrows()):\n",
        "                print(f\"   {i+1}. {row['cleaned_text'][:80]}...\")\n",
        "        else:\n",
        "            print(\"No data to display!\")\n",
        "\n",
        "preprocessor = PersianDatasetPreprocessor()\n",
        "\n",
        "df = preprocessor.upload_dataset()\n",
        "\n",
        "if df is not None:\n",
        "    preprocessor.analyze_dataset(df)\n",
        "\n",
        "    processed_df = preprocessor.preprocess(df)\n",
        "\n",
        "    preprocessor.show_samples(5)\n",
        "\n",
        "    print(\"\\nDataset is ready! You can use processed_df.\")\n",
        "else:\n",
        "    print(\"Operation stopped!\")"
      ],
      "metadata": {
        "id": "wQKMUrWXIa2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cleaned_tweets = preprocessor.processed_data['cleaned_text'].tolist()\n",
        "\n",
        "print(f\"Number of cleaned tweets: {len(cleaned_tweets)}\")\n",
        "\n",
        "preprocessor.processed_data.to_csv('cleaned_dataset.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('cleaned_dataset.csv')"
      ],
      "metadata": {
        "id": "tghg-P37Iaz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "model_name = \"HooshvareLab/bert-fa-base-uncased\"\n",
        "classifier = pipeline(\"text-generation\", model=model_name, tokenizer=model_name)\n",
        "\n",
        "prompt_template = '''ÙˆØ¸ÛŒÙÙ‡: ØªÙˆÛŒÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø¨Ù‡ Û³ Ú©Ù„Ø§Ø³ Ø¨Ø±Ú†Ø³Ø¨ Ø¨Ø²Ù†: 0=Ù…Ù†ÙÛŒØŒ1=Ø®Ù†Ø«ÛŒØŒ2=Ù…Ø«Ø¨Øª.\n",
        "Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§:\n",
        "\"Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„ Ø§ÙØªØ¶Ø§Ø­Ù‡ØŒ Ù¾ÙˆÙ„Ù…Ùˆ Ø¯ÙˆØ± Ø±ÛŒØ®ØªÙ…\" => 0\n",
        "\"Ø§Ù…Ø±ÙˆØ² Ù‡Ù… Ù…Ø«Ù„ Ø¯ÛŒØ±ÙˆØ² Ø¨ÙˆØ¯ØŒ Ú†ÛŒØ²ÛŒ Ø®Ø§Øµ Ù†Ø¨ÙˆØ¯\" => 1\n",
        "\"Ø¢ÙØ±ÛŒÙ†! Ú†Ù‡ Ø³Ø±ÙˆÛŒØ³ Ø³Ø±ÛŒØ¹ÛŒ Ø¯Ø§Ø´ØªÙ†Ø¯ ðŸ˜Š\" => 2\n",
        "\n",
        "Ù…ØªÙ†: \"{}\"\n",
        "Ù¾Ø§Ø³Ø® ÙÙ‚Ø· Ø¹Ø¯Ø¯ Ú©Ù„Ø§Ø³.'''\n",
        "\n",
        "\n",
        "tweets = [\n",
        "    \"Ø®ÛŒÙ„ÛŒ Ø§Ø² Ø§ÛŒÙ† Ù…Ø­ØµÙˆÙ„ Ø±Ø§Ø¶ÛŒ Ø¨ÙˆØ¯Ù… ðŸ˜\",\n",
        "    \"ÙˆØ§Ù‚Ø¹Ø§ Ø®Ø³ØªÙ‡ Ø´Ø¯Ù… Ø§Ø² Ø§ÛŒÙ† ÙˆØ¶Ø¹ÛŒØª\",\n",
        "    \"Ø§Ù…Ø±ÙˆØ² Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø¨ÙˆØ¯ØŒ Ù†Ù‡ Ø¨Ø¯ Ø¨ÙˆØ¯ Ù†Ù‡ Ø®ÙˆØ¨\"\n",
        "]\n",
        "\n",
        "pred_labels = []\n",
        "\n",
        "for tweet in tweets:\n",
        "    prompt = prompt_template.format(tweet)\n",
        "    output = classifier(prompt, max_new_tokens=5)[0]['generated_text']\n",
        "\n",
        "    match = re.search(r\"\\b([012])\\b\", output)\n",
        "    label = int(match.group(1)) if match else None\n",
        "    pred_labels.append(label)\n",
        "\n",
        "for t, l in zip(tweets, pred_labels):\n",
        "    print(f\"{t} => {l}\")\n"
      ],
      "metadata": {
        "id": "qRrgHRkFc9Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QCblrLjoc9Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xRgR-jM5c9Ep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}